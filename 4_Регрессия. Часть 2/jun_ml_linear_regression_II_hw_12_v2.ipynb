{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdghytLbBW9W"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTvgYjVhnE6h"
   },
   "source": [
    "### Урок 12. Домашняя работа\n",
    "\n",
    "**Задача высокого уровня** В реализацию функции `gradient` добавьте параметр $\\lambda$, чтобы получить регуляризованный градиентный спуск\n",
    "\n",
    "Формула поменяется следующим образом:\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial L}{\\partial w_0} = \\frac{2}{n}\\cdot(-1)\\cdot \\sum_{i=1}^{n} (1\\cdot \\left(y_i - \\sum_{j=1}^{m}w_jx_j^i\\right) + \\lambda\\cdot 2\\cdot w_0)&\\\\\n",
    "\\frac{\\partial L}{\\partial w_k} = \\frac{2}{n}\\cdot(-1)\\cdot \\sum_{i=1}^{n} (x_k^i \\cdot\\left(y_i - \\sum_{j=1}^{m}w_jx_j^i\\right) + \\lambda\\cdot 2\\cdot w_k)& k\\neq 0 \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zxpLYBR9PuBe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor,LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_train</th>\n",
       "      <th>y_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.182421</td>\n",
       "      <td>1.860341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.251605</td>\n",
       "      <td>1.878928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.270474</td>\n",
       "      <td>2.430015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.402553</td>\n",
       "      <td>2.327856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.427711</td>\n",
       "      <td>2.203649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x_train   y_train\n",
       "5  1.182421  1.860341\n",
       "6  1.251605  1.878928\n",
       "7  1.270474  2.430015\n",
       "8  1.402553  2.327856\n",
       "9  1.427711  2.203649"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('non_linear.csv', sep=',')\n",
    "data = data[(data.x_train > 1) & (data.x_train < 5)].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **обычный градиентный спуск**\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial L}{\\partial w_0} = \\frac{2}{n}\\cdot(-1)\\cdot \\sum_{i=1}^{n} (1\\cdot \\left(y_i - \\sum_{j=1}^{m}w_jx_j^i\\right))&\\\\\n",
    "\\frac{\\partial L}{\\partial w_k} = \\frac{2}{n}\\cdot(-1)\\cdot \\sum_{i=1}^{n} (x_k^i \\cdot\\left(y_i - \\sum_{j=1}^{m}w_jx_j^i\\right))& k\\neq 0 \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent (X, y, eta = 0.008, epsilon = 0.001, fit_intercept = True):\n",
    "    \"Функция обычного градиетного спуска\"\n",
    "           \n",
    "    step = 0\n",
    "    weight_evolution = np.inf\n",
    "    # количество обучающих примеров в выборке\n",
    "    n = X.shape[0]\n",
    "    # количество признаков\n",
    "    m = X.shape[1] \n",
    "\n",
    "    if fit_intercept == True:\n",
    "        # добавляем тривиальный признак w_0, столбец из единиц\n",
    "        X = np.hstack([np.ones(n).reshape(-1,1), X])\n",
    "        # количество признаков\n",
    "        m = X.shape[1]\n",
    "        \n",
    "    # инициализируем рандомом веса\n",
    "    w=[]\n",
    "    w = np.random.random(m).reshape(1, -1)\n",
    "        \n",
    "    while weight_evolution > epsilon:  # повторяем до сходимости вектора весов\n",
    "        step += 1\n",
    "\n",
    "        # считаем прогноз\n",
    "        y_hat = X.dot(w.T)\n",
    "        # вычисляем ошибку прогноза\n",
    "        error = y - y_hat\n",
    "        # вычисляем градиент дальше pointwise перемножение - умножаем каждую из координат на ошибку\n",
    "        grad = np.multiply(X, error).sum(axis=0)*(-1.0)*2.0 / n\n",
    "        # делаем шаг градиентного спуска\n",
    "        w_next = w - eta * grad\n",
    "        # проверяем условие сходимости\n",
    "        weight_evolution = distance.euclidean(w, w_next)\n",
    "        w = w_next\n",
    "    \n",
    "        if step % 100 ==0:\n",
    "            print(\"step %s |w-w_next|=%.5f, grad=%s\" % (step, weight_evolution, grad))\n",
    "    \n",
    "        if step > 5000: \n",
    "            break\n",
    "    return w\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100 |w-w_next|=0.00511, grad=[-0.49124676  0.14191142]\n",
      "step 200 |w-w_next|=0.00401, grad=[-0.38501243  0.11122243]\n",
      "step 300 |w-w_next|=0.00314, grad=[-0.30175175  0.08717008]\n",
      "step 400 |w-w_next|=0.00246, grad=[-0.23649657  0.06831916]\n",
      "step 500 |w-w_next|=0.00193, grad=[-0.18535312  0.05354483]\n",
      "step 600 |w-w_next|=0.00151, grad=[-0.14526967  0.04196552]\n",
      "step 700 |w-w_next|=0.00119, grad=[-0.11385445  0.03289029]\n"
     ]
    }
   ],
   "source": [
    "# трансформируем плоский массив X в вектор-столбец\n",
    "X = data['x_train'].values.reshape(-1, 1)\n",
    "# n = X.shape[0]\n",
    "# добавляем тривиальный признак w_0, столбец из единиц\n",
    "# X = np.hstack([np.ones(n).reshape(-1,1), X])\n",
    "y = data['y_train'].values.reshape(-1, 1)\n",
    "\n",
    "a = True # fit_intercept\n",
    "W = gradient_descent (X, y, eta=0.01, fit_intercept = a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12241906910716566"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_intercept = a\n",
    "if fit_intercept == True:\n",
    "    X = data['x_train'].values.reshape(-1, 1)\n",
    "    n = X.shape[0]\n",
    "    X = np.hstack([np.ones(n).reshape(-1,1), X])\n",
    "else:\n",
    "    X = data['x_train'].values.reshape(-1, 1)\n",
    "    \n",
    "y_pred = X.dot(W.T)\n",
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17953174261309246"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка - значение mse не совпадает со значением, полученным, когда реализую градиентный спуск вручную\n",
    "sgd_regressor = SGDRegressor(learning_rate = 'constant', eta0 = 0.01, fit_intercept = a, random_state = 42)\n",
    "reg = SGDRegressor().fit(X, y)\n",
    "y_pred = reg.predict(X)\n",
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **градиентный спуск с регуляризацией**\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{cc}\n",
    "\\frac{\\partial L}{\\partial w_0} = \\frac{2}{n}\\cdot(-1)\\cdot \\sum_{i=1}^{n} (1\\cdot \\left(y_i - \\sum_{j=1}^{m}w_jx_j^i\\right) + \\lambda\\cdot 2\\cdot w_0)&\\\\\n",
    "\\frac{\\partial L}{\\partial w_k} = \\frac{2}{n}\\cdot(-1)\\cdot \\sum_{i=1}^{n} (x_k^i \\cdot\\left(y_i - \\sum_{j=1}^{m}w_jx_j^i\\right) + \\lambda\\cdot 2\\cdot w_k)& k\\neq 0 \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_reg (X, y, eta = 0.008, epsilon = 0.001, l = 0.01, fit_intercept = True):\n",
    "    \"Функция градиетного спуска с регуляризацией L2\"\n",
    "    \n",
    "    step = 0\n",
    "    weight_evolution = np.inf\n",
    "    # количество обучающих примеров в выборке\n",
    "    n = X.shape[0]\n",
    "    # количество признаков\n",
    "    m = X.shape[1]\n",
    "\n",
    "    if fit_intercept == True:\n",
    "        # добавляем тривиальный признак w_0, столбец из единиц\n",
    "        X = np.hstack([np.ones(n).reshape(-1,1), X])\n",
    "        # количество признаков\n",
    "        m = X.shape[1]\n",
    "        \n",
    "    # инициализируем рандомом веса\n",
    "    w=[]\n",
    "    w = np.random.random(m).reshape(1, -1)\n",
    "    \n",
    "    while weight_evolution > epsilon:  # повторяем до сходимости вектора весов\n",
    "        step += 1\n",
    "\n",
    "        # считаем прогноз\n",
    "        y_hat = X.dot(w.T)\n",
    "        # вычисляем ошибку прогноза\n",
    "        error = y - y_hat\n",
    "        # вычисляем градиент дальше pointwise перемножение - умножаем каждую из координат на ошибку\n",
    "        grad = (np.multiply(X, error) + 2 * l * w).sum(axis=0)*(-1.0)*2.0 / n \n",
    "        # делаем шаг градиентного спуска\n",
    "        w_next = w - eta * grad\n",
    "        # проверяем условие сходимости\n",
    "        weight_evolution = distance.euclidean(w, w_next)\n",
    "        w = w_next\n",
    "    \n",
    "        if step % 100 ==0:\n",
    "            print(\"step %s |w-w_next|=%.5f, grad=%s\" % (step, weight_evolution, grad))\n",
    "    \n",
    "        if step > 5000: \n",
    "            break\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100 |w-w_next|=0.00443, grad=[-0.42547571  0.12291147]\n",
      "step 200 |w-w_next|=0.00347, grad=[-0.33346466  0.09633131]\n",
      "step 300 |w-w_next|=0.00272, grad=[-0.26135143  0.07549923]\n",
      "step 400 |w-w_next|=0.00213, grad=[-0.204833    0.05917218]\n",
      "step 500 |w-w_next|=0.00167, grad=[-0.16053694  0.04637593]\n",
      "step 600 |w-w_next|=0.00131, grad=[-0.1258201   0.03634692]\n",
      "step 700 |w-w_next|=0.00103, grad=[-0.09861094  0.02848674]\n"
     ]
    }
   ],
   "source": [
    "# трансформируем плоский массив X в вектор-столбец\n",
    "X = data['x_train'].values.reshape(-1, 1)\n",
    "# n = X.shape[0]\n",
    "# добавляем тривиальный признак w_0, столбец из единиц\n",
    "# X = np.hstack([np.ones(n).reshape(-1,1), X])\n",
    "y = data['y_train'].values.reshape(-1, 1)\n",
    "\n",
    "a = True # fit_intercept\n",
    "W = gradient_descent (X, y, eta=0.01, fit_intercept = a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1224201776980459"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_intercept = a\n",
    "if fit_intercept == True:\n",
    "    X = data['x_train'].values.reshape(-1, 1)\n",
    "    n = X.shape[0]\n",
    "    X = np.hstack([np.ones(n).reshape(-1,1), X])\n",
    "else:\n",
    "    X = data['x_train'].values.reshape(-1, 1)\n",
    "    \n",
    "y_pred = X.dot(W.T)\n",
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1804080936393841"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверка - значение mse не совпадает со значением, полученным, когда реализую градиентный спуск вручную\n",
    "sgd_regressor = SGDRegressor(penalty = 'l2', alpha = 0.01, learning_rate = 'constant', eta0 = 0.01,\\\n",
    "                             fit_intercept = a, random_state = 42)\n",
    "reg = SGDRegressor().fit(X, y)\n",
    "y_pred = reg.predict(X)\n",
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQ7gRtDmnE6p"
   },
   "source": [
    "В этом модуле мы узнали, как  обучать линейную регрессию, не \"упираясь\" в аппаратные ресурсы: использовать градиентный спуск.\n",
    "Мы узнали, как детектировать переобучение модели и закрепили свои знания на примере полиномиальной регрессии и выяснили, как увеличить качество решения с помощью механизма регуляризации. Познакомились с двумя видами регуляризации -  Ridge и Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "jun_ml_linear_regression_II-hw_12.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
